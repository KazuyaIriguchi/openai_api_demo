{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# チャットモデルで関数を呼び出す方法"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functionsの使い方\n",
    "\n",
    "ChatCompletion APIのオプションパラメータで、関数の仕様を提供するために使用する。\n",
    "目的は、モデルが関数入力スキーマに準拠した出力を生成することを可能にすること。\n",
    "\n",
    "**※APIは実際に関数の呼び出しを実行することはない**\n",
    "\n",
    "モデルの出力を使って関数呼び出しを実行するのは、開発者自身"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "関数のパラメータ定義：\n",
    "\n",
    "| param | detail |\n",
    "| :--- | :--- |\n",
    "| Name | 関数の名前 |\n",
    "| Description |関数が何をするのかの説明。モデルはこれを用いて、関数を呼び出すタイミングを決定する。 |\n",
    "| Parameters |parametersオブジェクトは、関数が必要とするすべての入力フィールドを含んでいる。（String, Number, Boolean, Object, Null, AnyOf） |\n",
    "| Required |クエリを作成するために必要なパラメータを指定する。指定しなかった残りのパラメータはオプションとして扱われる。 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ワークフロー\n",
    "\n",
    "1. 基本的なコンセプト：サンプル関数を作成し、適切な場合はAPIにそれを使用させる。\n",
    "1. APIコールと関数の実行を統合する：APIコールを使って関数の引数を生成し、関数を実行するエージェントを作成する。\n",
    "1. 複数の関数を使用する：ユーザーに応答する前に、複数の関数を順番に呼び出すことができるようにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in .\\.venv\\lib\\site-packages (0.27.8)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tqdm in .\\.venv\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: requests>=2.20 in .\\.venv\\lib\\site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: aiohttp in .\\.venv\\lib\\site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\.venv\\lib\\site-packages (from requests>=2.20->openai) (2.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\.venv\\lib\\site-packages (from requests>=2.20->openai) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\.venv\\lib\\site-packages (from requests>=2.20->openai) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\.venv\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in .\\.venv\\lib\\site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in .\\.venv\\lib\\site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in .\\.venv\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in .\\.venv\\lib\\site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in .\\.venv\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in .\\.venv\\lib\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: colorama in .\\.venv\\lib\\site-packages (from tqdm->openai) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'd:\\work_space\\github\\openai_api_demo\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tenacity in .\\.venv\\lib\\site-packages (8.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'd:\\work_space\\github\\openai_api_demo\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in .\\.venv\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\.venv\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\.venv\\lib\\site-packages (from requests) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\.venv\\lib\\site-packages (from requests) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\.venv\\lib\\site-packages (from requests) (2.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'd:\\work_space\\github\\openai_api_demo\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: termcolor in .\\.venv\\lib\\site-packages (2.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'd:\\work_space\\github\\openai_api_demo\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai\n",
    "%pip install tenacity\n",
    "%pip install requests\n",
    "%pip install termcolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from termcolor import colored\n",
    "import requests\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "\n",
    "GPT_MODEL = \"gpt-3.5-turbo-0613\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility\n",
    "# ChatCompletionを呼び出す関数\n",
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def chat_completion_request(messages, functions=None, model=GPT_MODEL):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer \" + openai.api_key,\n",
    "    }\n",
    "    json_data = {\"model\": model, \"messages\": messages}\n",
    "    if functions is not None:\n",
    "        json_data.update({\"functions\": functions})\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"https://api.openai.com/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=json_data,\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(\"Unable to generate ChatCompletion response\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 会話を管理するクラス\n",
    "class Conversation:\n",
    "    def __init__(self):\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        message = {\"role\": role, \"content\": content}\n",
    "        self.conversation_history.append(message)\n",
    "\n",
    "    def display_conversation(self, detailed=False):\n",
    "        role_to_color = {\n",
    "            \"system\": \"red\",\n",
    "            \"user\": \"green\",\n",
    "            \"assistant\": \"blue\",\n",
    "            \"function\": \"magenta\",\n",
    "        }\n",
    "        for message in self.conversation_history:\n",
    "            print(\n",
    "                colored(\n",
    "                    f\"{message['role']}: {message['content']}\\n\\n\",\n",
    "                    role_to_color[message[\"role\"]],\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## サンプル：天気を取得するメソッドの引数生成\n",
    "\n",
    "1. `get_current_weather`という関数の仕様書を作成\n",
    "1. 関数仕様をAPIに渡し、仕様に沿った関数の引数を生成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 天気を取得するメソッド\n",
    "import json\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    weather_info = {\n",
    "        \"location\": location,\n",
    "        \"temperature\": \"72\",\n",
    "        \"unit\": unit,\n",
    "        \"forecast\": [\"sunny\", \"windy\"],\n",
    "    }\n",
    "    return json.dumps(weather_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                },\n",
    "                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = Conversation()\n",
    "conversation.add_message(\"user\", \"what is the weather like today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant_message={'role': 'assistant', 'content': 'Sure, could you please provide me with your current location?'}\n"
     ]
    }
   ],
   "source": [
    "# 「今日の天気を教えて」と入力\n",
    "# ChatGPTは「`get_current_weather`に必要な`location`を教えてくれ」と回答する\n",
    "chat_response = chat_completion_request(\n",
    "    conversation.conversation_history, functions=functions\n",
    ")\n",
    "assistant_message = chat_response.json()[\"choices\"][0][\"message\"]\n",
    "conversation.add_message(assistant_message[\"role\"], assistant_message[\"content\"])\n",
    "print(f\"{assistant_message=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_response.json()['choices'][0]={'index': 0, 'message': {'role': 'assistant', 'content': None, 'function_call': {'name': 'get_current_weather', 'arguments': '{\\n  \"location\": \"Glasgow, Scotland\"\\n}'}}, 'finish_reason': 'function_call'}\n"
     ]
    }
   ],
   "source": [
    "# 必要な情報を提供→関数の実行に必要なパラメータを返す\n",
    "conversation.add_message(\"user\", \"I'm in Glasgow, Scotland\")\n",
    "chat_response = chat_completion_request(\n",
    "    conversation.conversation_history, functions=functions\n",
    ")\n",
    "print(f\"{chat_response.json()['choices'][0]=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"location\": \"Glasgow, Scotland\", \"temperature\": \"72\", \"unit\": null, \"forecast\": [\"sunny\", \"windy\"]}\n"
     ]
    }
   ],
   "source": [
    "# 関数実行\n",
    "message = chat_response.json()[\"choices\"][0][\"message\"]\n",
    "\n",
    "if message.get(\"function_call\"):\n",
    "    function_call = message[\"function_call\"]\n",
    "    function_name = function_call[\"name\"]\n",
    "    arguments = json.loads(function_call[\"arguments\"])\n",
    "\n",
    "    function_response = get_current_weather(\n",
    "        location=arguments.get(\"location\"),\n",
    "        unit=arguments.get(\"unit\"),\n",
    "    )\n",
    "    print(function_response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 複数の関数の使用\n",
    "\n",
    "モデルに複数の関数を呼び出して提供するシナリオ\n",
    "\n",
    "## シナリオ例：\n",
    "\n",
    "arXivのデータを使って、学術的なテーマに関する質問に答えるエージェントを作成する。このエージェントは、2つの新しい関数を自由に使うことができる。\n",
    "\n",
    "- `get_articles`: あるテーマに関するarXivの論文を取得し、リンク付きでユーザーのために要約する関数\n",
    "- `read_article_and_summarize`: 以前に検索された論文の1つを取り、その全体を読み、核となる議論、証拠、結論を要約する\n",
    "\n",
    "これにより、複数のサービスから選択できる多機能ワークフローになれることができ、最初の機能からのデータの一部を永続化して2番目の機能で使用することができる。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## arXiv検索\n",
    "\n",
    "まず、2つの機能を支えるユーティリティをいくつか設定します。\n",
    "\n",
    "ダウンロードした論文は、ディレクトリ（ここでは`./data/papers`を使用）に保存される。\n",
    "\n",
    "ダウンロードした論文の埋め込みと詳細を保存するために、`arxiv_library.csv`というファイルを作成し、`summarize_text`を使って検索するようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in .\\.venv\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in .\\.venv\\lib\\site-packages (from scipy) (1.24.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'd:\\work_space\\github\\openai_api_demo\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in .\\.venv\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in .\\.venv\\lib\\site-packages (from tiktoken) (2023.6.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in .\\.venv\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'd:\\work_space\\github\\openai_api_demo\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in .\\.venv\\lib\\site-packages (1.4.7)\n",
      "Requirement already satisfied: feedparser in .\\.venv\\lib\\site-packages (from arxiv) (6.0.10)\n",
      "Requirement already satisfied: sgmllib3k in .\\.venv\\lib\\site-packages (from feedparser->arxiv) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'd:\\work_space\\github\\openai_api_demo\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in .\\.venv\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in .\\.venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in .\\.venv\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in .\\.venv\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in .\\.venv\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in .\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'd:\\work_space\\github\\openai_api_demo\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in .\\.venv\\lib\\site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'd:\\work_space\\github\\openai_api_demo\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install scipy\n",
    "%pip install tiktoken\n",
    "%pip install arxiv\n",
    "%pip install pandas\n",
    "%pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import ast\n",
    "import concurrent\n",
    "from csv import writer\n",
    "from IPython.display import display, Markdown, Latex\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "import requests\n",
    "from scipy import spatial\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "\n",
    "GPT_MODEL = \"gpt-3.5-turbo-0613\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a directory to store downloaded papers\n",
    "data_dir = os.path.join(os.curdir, \"data\", \"papers\")\n",
    "paper_dir_filepath = \"./data/arxiv_library.csv\"\n",
    "\n",
    "# Generate a blank dataframe where we can store downloaded files\n",
    "df = pd.DataFrame(list())\n",
    "df.to_csv(paper_dir_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def embedding_request(text):\n",
    "    response = openai.Embedding.create(input=text, model=EMBEDDING_MODEL)\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_articles(query, library=paper_dir_filepath, top_k=5):\n",
    "    \"\"\"This function gets the top_k articles based on a user's query, sorted by relevance.\n",
    "    It also downloads the files and stores them in arxiv_library.csv to be retrieved by the read_article_and_summarize.\n",
    "    \"\"\"\n",
    "    search = arxiv.Search(\n",
    "        query=query, max_results=top_k, sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    result_list = []\n",
    "    for result in search.results():\n",
    "        result_dict = {}\n",
    "        result_dict.update({\"title\": result.title})\n",
    "        result_dict.update({\"summary\": result.summary})\n",
    "\n",
    "        # Taking the first url provided\n",
    "        result_dict.update({\"article_url\": [x.href for x in result.links][0]})\n",
    "        result_dict.update({\"pdf_url\": [x.href for x in result.links][1]})\n",
    "        result_list.append(result_dict)\n",
    "\n",
    "        # Store references in library file\n",
    "        response = embedding_request(text=result.title)\n",
    "        file_reference = [\n",
    "            result.title,\n",
    "            result.download_pdf(data_dir),\n",
    "            response[\"data\"][0][\"embedding\"],\n",
    "        ]\n",
    "\n",
    "        # Write to file\n",
    "        with open(library, \"a\") as f_object:\n",
    "            writer_object = writer(f_object)\n",
    "            writer_object.writerow(file_reference)\n",
    "            f_object.close()\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Proximal Policy Optimization and its Dynamic Version for Sequence Generation',\n",
       " 'summary': 'In sequence generation task, many works use policy gradient for model\\noptimization to tackle the intractable backpropagation issue when maximizing\\nthe non-differentiable evaluation metrics or fooling the discriminator in\\nadversarial learning. In this paper, we replace policy gradient with proximal\\npolicy optimization (PPO), which is a proved more efficient reinforcement\\nlearning algorithm, and propose a dynamic approach for PPO (PPO-dynamic). We\\ndemonstrate the efficacy of PPO and PPO-dynamic on conditional sequence\\ngeneration tasks including synthetic experiment and chit-chat chatbot. The\\nresults show that PPO and PPO-dynamic can beat policy gradient by stability and\\nperformance.',\n",
       " 'article_url': 'http://arxiv.org/abs/1808.07982v1',\n",
       " 'pdf_url': 'http://arxiv.org/pdf/1808.07982v1'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test that the search is working\n",
    "result_output = get_articles(\"ppo reinforcement learning\")\n",
    "result_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strings_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "    top_n: int = 100,\n",
    ") -> list[str]:\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    query_embedding_response = embedding_request(query)\n",
    "    query_embedding = query_embedding_response[\"data\"][0][\"embedding\"]\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"filepath\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(filepath):\n",
    "    \"\"\"Takes a filepath to a PDF and returns a string of the PDF's contents\"\"\"\n",
    "    # creating a pdf reader object\n",
    "    reader = PdfReader(filepath)\n",
    "    pdf_text = \"\"\n",
    "    page_number = 0\n",
    "    for page in reader.pages:\n",
    "        page_number += 1\n",
    "        pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
    "    return pdf_text\n",
    "\n",
    "\n",
    "# Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
    "def create_chunks(text, n, tokenizer):\n",
    "    \"\"\"Returns successive n-sized chunks from provided text.\"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
    "        j = min(i + int(1.5 * n), len(tokens))\n",
    "        while j > i + int(0.5 * n):\n",
    "            # Decode the tokens and check for full stop or newline\n",
    "            chunk = tokenizer.decode(tokens[i:j])\n",
    "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
    "                break\n",
    "            j -= 1\n",
    "        # If no end of sentence found, use n tokens as the chunk size\n",
    "        if j == i + int(0.5 * n):\n",
    "            j = min(i + n, len(tokens))\n",
    "        yield tokens[i:j]\n",
    "        i = j\n",
    "\n",
    "\n",
    "def extract_chunk(content, template_prompt):\n",
    "    \"\"\"This function applies a prompt to some input content. In this case it returns a summarize chunk of text\"\"\"\n",
    "    prompt = template_prompt + content\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=GPT_MODEL, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def summarize_text(query):\n",
    "    \"\"\"This function does the following:\n",
    "    - Reads in the arxiv_library.csv file in including the embeddings\n",
    "    - Finds the closest file to the user's query\n",
    "    - Scrapes the text out of the file and chunks it\n",
    "    - Summarizes each chunk in parallel\n",
    "    - Does one final summary and returns this to the user\"\"\"\n",
    "\n",
    "    # A prompt to dictate how the recursive summarizations should approach the input paper\n",
    "    summary_prompt = \"\"\"Summarize this text from an academic paper. Extract any key points with reasoning.\\n\\nContent:\"\"\"\n",
    "\n",
    "    # If the library is empty (no searches have been performed yet), we perform one and download the results\n",
    "    library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
    "    if len(library_df) == 0:\n",
    "        print(\"No papers searched yet, downloading first.\")\n",
    "        get_articles(query)\n",
    "        print(\"Papers downloaded, continuing\")\n",
    "        library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
    "    library_df.columns = [\"title\", \"filepath\", \"embedding\"]\n",
    "    library_df[\"embedding\"] = library_df[\"embedding\"].apply(ast.literal_eval)\n",
    "    strings = strings_ranked_by_relatedness(query, library_df, top_n=1)\n",
    "    print(\"Chunking text from paper\")\n",
    "    pdf_text = read_pdf(strings[0])\n",
    "\n",
    "    # Initialise tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    results = \"\"\n",
    "\n",
    "    # Chunk up the document into 1500 token chunks\n",
    "    chunks = create_chunks(pdf_text, 1500, tokenizer)\n",
    "    text_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
    "    print(\"Summarizing each chunk of text\")\n",
    "\n",
    "    # Parallel process the summaries\n",
    "    with concurrent.futures.ThreadPoolExecutor(\n",
    "        max_workers=len(text_chunks)\n",
    "    ) as executor:\n",
    "        futures = [\n",
    "            executor.submit(extract_chunk, chunk, summary_prompt)\n",
    "            for chunk in text_chunks\n",
    "        ]\n",
    "        with tqdm(total=len(text_chunks)) as pbar:\n",
    "            for _ in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(1)\n",
    "        for future in futures:\n",
    "            data = future.result()\n",
    "            results += data\n",
    "\n",
    "    # Final summary\n",
    "    print(\"Summarizing into overall summary\")\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=GPT_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Write a summary collated from this collection of key points extracted from an academic paper.\n",
    "                        The summary should highlight the core argument, conclusions and evidence, and answer the user's query.\n",
    "                        User query: {query}\n",
    "                        The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\n",
    "                        Key points:\\n{results}\\nSummary:\\n\"\"\",\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking text from paper\n",
      "Summarizing each chunk of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:09<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing into overall summary\n"
     ]
    }
   ],
   "source": [
    "# summarize_text 関数のテスト\n",
    "chat_test_response = summarize_text(\"PPO reinforcement learning sequence generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core Argument:\n",
      "- The paper discusses the use of Proximal Policy Optimization (PPO) in sequence generation tasks, specifically in the context of chit-chat chatbots.\n",
      "- The authors argue that PPO is a more efficient reinforcement learning algorithm compared to policy gradient, which is commonly used in these tasks.\n",
      "- They propose a dynamic approach for PPO (PPO-dynamic) and demonstrate its efficacy in synthetic experiments and chit-chat chatbot tasks.\n",
      "\n",
      "Evidence:\n",
      "- PPO-dynamic achieves a high precision score in a synthetic counting task, comparable to other algorithms like REINFORCE and MIXER.\n",
      "- In the chit-chat chatbot task, PPO-dynamic achieves a slightly higher BLEU-2 score than REINFORCE and PPO.\n",
      "- The learning curves of PPO and PPO-dynamic are more stable than policy gradient, and PPO-dynamic converges faster.\n",
      "\n",
      "Conclusions:\n",
      "- PPO is a better optimization method for sequence learning compared to policy gradient.\n",
      "- PPO-dynamic further improves the optimization process by dynamically adjusting the hyperparameters.\n",
      "- PPO can be used as a new optimization method for GAN-based sequence learning for better performance.\n"
     ]
    }
   ],
   "source": [
    "print(chat_test_response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エージェントの設定\n",
    "\n",
    "arXivデータへのアクセスを提供する関数について、2つの関数仕様を作成します。また、`Chat Completions`API呼び出しと関数実行を統合するためのユーティリティもいくつか作成する予定です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_articlesとread_article_and_summarize関数を開始\n",
    "arxiv_functions = [\n",
    "    {\n",
    "        \"name\": \"get_articles\",\n",
    "        \"description\": \"\"\"Use this function to get academic papers from arXiv to answer user questions.\"\"\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": f\"\"\"\n",
    "                            User query in JSON. Responses should be summarized and should include the article URL reference\n",
    "                            \"\"\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "        \"name\": \"read_article_and_summarize\",\n",
    "        \"description\": \"\"\"Use this function to read whole papers and provide a summary for users.\n",
    "        You should NEVER call this function before get_articles has been called in the conversation.\"\"\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": f\"\"\"\n",
    "                            Description of the article in plain text based on the user's query\n",
    "                            \"\"\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_completion_with_function_execution(messages, functions=[None]):\n",
    "    \"\"\"This function makes a ChatCompletion API call with the option of adding functions\"\"\"\n",
    "    response = chat_completion_request(messages, functions)\n",
    "    print(response.json())\n",
    "    full_message = response.json()[\"choices\"][0]\n",
    "    if full_message[\"finish_reason\"] == \"function_call\":\n",
    "        print(f\"Function generation requested, calling function\")\n",
    "        return call_arxiv_function(messages, full_message)\n",
    "    else:\n",
    "        print(f\"Function not required, responding to user\")\n",
    "        return response.json()\n",
    "\n",
    "\n",
    "def call_arxiv_function(messages, full_message):\n",
    "    \"\"\"Function calling function which executes function calls when the model believes it is necessary.\n",
    "    Currently extended by adding clauses to this if statement.\"\"\"\n",
    "\n",
    "    if full_message[\"message\"][\"function_call\"][\"name\"] == \"get_articles\":\n",
    "        try:\n",
    "            parsed_output = json.loads(\n",
    "                full_message[\"message\"][\"function_call\"][\"arguments\"]\n",
    "            )\n",
    "            print(\"Getting search results\")\n",
    "            results = get_articles(parsed_output[\"query\"])\n",
    "        except Exception as e:\n",
    "            print(parsed_output)\n",
    "            print(f\"Function execution failed\")\n",
    "            print(f\"Error message: {e}\")\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"function\",\n",
    "                \"name\": full_message[\"message\"][\"function_call\"][\"name\"],\n",
    "                \"content\": str(results),\n",
    "            }\n",
    "        )\n",
    "        try:\n",
    "            print(\"Got search results, summarizing content\")\n",
    "            response = chat_completion_request(messages)\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            print(type(e))\n",
    "            raise Exception(\"Function chat request failed\")\n",
    "\n",
    "    elif (\n",
    "        full_message[\"message\"][\"function_call\"][\"name\"] == \"read_article_and_summarize\"\n",
    "    ):\n",
    "        parsed_output = json.loads(\n",
    "            full_message[\"message\"][\"function_call\"][\"arguments\"]\n",
    "        )\n",
    "        print(\"Finding and reading paper\")\n",
    "        summary = summarize_text(parsed_output[\"query\"])\n",
    "        return summary\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Function does not exist and cannot be called\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arXivの会話\n",
    "\n",
    "会話で機能を試してみよう\n",
    "\n",
    "#### シナリオ\n",
    "\n",
    "まず、システムメッセージで以下を送信（DeepL訳）\n",
    "\n",
    "> あなたはarXivGPTです。ユーザーの質問に答えるために、学術論文を引っ張ってくる便利なアシスタントです。\n",
    "あなたは論文を明確に要約し、顧客が自分の質問に答えるためにどれを読むべきかを判断できるようにします。\n",
    "ユーザーが論文の名前を理解し、クリックしてアクセスできるように、あなたは常に記事のURLとタイトルを提供します。\n",
    "始める！\n",
    "\n",
    "次にユーザーメッセージを送信\n",
    "\n",
    "> こんにちは、PPO強化学習とはどのようなものなのでしょうか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a system message\n",
    "paper_system_message = \"\"\"You are arXivGPT, a helpful assistant pulls academic papers to answer user questions.\n",
    "You summarize the papers clearly so the customer can decide which to read to answer their question.\n",
    "You always provide the article_url and title so the user can understand the name of the paper and click through to access it.\n",
    "Begin!\"\"\"\n",
    "paper_conversation = Conversation()\n",
    "paper_conversation.add_message(\"system\", paper_system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-7RIABC8Ihn7jep86eSFW3YnUA1Cqg', 'object': 'chat.completion', 'created': 1686739031, 'model': 'gpt-3.5-turbo-0613', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': None, 'function_call': {'name': 'read_article_and_summarize', 'arguments': '{\\n  \"query\": \"PPO reinforcement learning\"\\n}'}}, 'finish_reason': 'function_call'}], 'usage': {'prompt_tokens': 163, 'completion_tokens': 22, 'total_tokens': 185}}\n",
      "Function generation requested, calling function\n",
      "Finding and reading paper\n",
      "Chunking text from paper\n",
      "Summarizing each chunk of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:10<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing into overall summary\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Core Argument:\n",
       "- The paper focuses on the theoretical analysis of the PPO-Clip algorithm in the context of deep reinforcement learning.\n",
       "- The authors propose two core ideas: reinterpreting PPO-Clip from the perspective of hinge loss and introducing a two-step policy improvement scheme.\n",
       "- The paper establishes the global convergence of PPO-Clip and characterizes its convergence rate as O(1/sqrt(T)).\n",
       "- The empirical experiments validate the effectiveness of PPO-Clip on various reinforcement learning benchmark tasks.\n",
       "\n",
       "Evidence:\n",
       "- The paper addresses the challenges posed by the clipping mechanism and neural function approximation in analyzing PPO-Clip.\n",
       "- The authors provide theoretical proofs, lemmas, and corollaries to support their claims.\n",
       "- The experiments on different classifiers and benchmark tasks demonstrate the performance of PPO-Clip.\n",
       "\n",
       "Conclusions:\n",
       "- The paper offers theoretical insights into the performance of PPO-Clip and provides a framework for analyzing its convergence properties.\n",
       "- The results contribute to a better understanding of PPO-Clip in the reinforcement learning community.\n",
       "- PPO-Clip is an effective algorithm for policy improvement in deep reinforcement learning."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add a user message\n",
    "paper_conversation.add_message(\"user\", \"Hi, how does PPO reinforcement learning work?\")\n",
    "chat_response = chat_completion_with_function_execution(\n",
    "    paper_conversation.conversation_history, functions=arxiv_functions\n",
    ")\n",
    "assistant_message = chat_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "paper_conversation.add_message(\"assistant\", assistant_message)\n",
    "display(Markdown(assistant_message))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "シナリオを実行すると、「PPO強化学習」に関する論文を必要に応じてダウンロードし、中身を解析、要約する。\n",
    "\n",
    "要約した内容\n",
    "\n",
    "---\n",
    "\n",
    "核となる主張\n",
    "- 本論文は、深層強化学習におけるPPO-Clipアルゴリズムの理論解析に焦点を当てたものである。\n",
    "- 神経関数近似の下でのPPO-Clipの最初のグローバルな収束率保証を提供することを目的としている。\n",
    "\n",
    "根拠を示す：\n",
    "- 著者らは、方針更新の閉形式の欠如や、クリッピング動作と近似誤差の結合など、PPO-Clipを分析する上での課題を明らかにしている。\n",
    "- 彼らは、ヒンジ損失の観点からPPO-Clipを再解釈することと、2段階の政策改善スキームを導入することという、2つの核となるアイデアを提案する。\n",
    "- 本論文では、PPO-Clipのグローバルな収束性を確立し、その収束率をO(1/sqrt(T))として特徴付け、実験を通じて再解釈を検証する。\n",
    "\n",
    "結論\n",
    "- 本論文は、PPO-Clipの性能に関する理論的な洞察を提供し、その収束特性を分析するためのフレームワークを提供するものである。\n",
    "- 強化学習コミュニティにおけるPPO-Clipのより良い理解に貢献するものである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-7RIDQDmEbYqZm7tVS4SLnkyJrzTil', 'object': 'chat.completion', 'created': 1686739232, 'model': 'gpt-3.5-turbo-0613', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': None, 'function_call': {'name': 'read_article_and_summarize', 'arguments': '{\\n  \"query\": \"PPO sequence generation\"\\n}'}}, 'finish_reason': 'function_call'}], 'usage': {'prompt_tokens': 412, 'completion_tokens': 22, 'total_tokens': 434}}\n",
      "Function generation requested, calling function\n",
      "Finding and reading paper\n",
      "Chunking text from paper\n",
      "Summarizing each chunk of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:08<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing into overall summary\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Core Argument:\n",
       "The paper discusses the use of Proximal Policy Optimization (PPO) in sequence generation tasks, specifically in the context of chit-chat chatbots. The authors argue that PPO is a more efficient reinforcement learning algorithm compared to policy gradient, which is commonly used in text generation tasks. They propose a dynamic approach for PPO (PPO-dynamic) and demonstrate its efficacy in synthetic experiments and chit-chat chatbot tasks.\n",
       "\n",
       "Evidence:\n",
       "- The authors derive the constraints for PPO-dynamic and provide the pseudo code for both PPO and PPO-dynamic.\n",
       "- In synthetic experiments, PPO-dynamic achieves a high precision score comparable to other algorithms.\n",
       "- In the chit-chat chatbot task, PPO-dynamic achieves a slightly higher BLEU-2 score than other algorithms.\n",
       "- The learning curves of PPO and PPO-dynamic are more stable than policy gradient, and PPO-dynamic converges faster.\n",
       "\n",
       "Conclusions:\n",
       "- PPO is a better optimization method for sequence learning compared to policy gradient.\n",
       "- PPO-dynamic further improves the optimization process by dynamically adjusting the hyperparameters.\n",
       "- PPO can be used as a new optimization method for GAN-based sequence learning for better performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# システムに2つ目のツールを使わせるために、もう1つのユーザーメッセージを追加する。\n",
    "paper_conversation.add_message(\n",
    "    \"user\",\n",
    "    \"Can you read the PPO sequence generation paper for me and give me a summary\",\n",
    ")\n",
    "updated_response = chat_completion_with_function_execution(\n",
    "    paper_conversation.conversation_history, functions=arxiv_functions\n",
    ")\n",
    "display(Markdown(updated_response[\"choices\"][0][\"message\"][\"content\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力\n",
    "- PPO配列生成の論文を代読し、概要を教えてください\n",
    "\n",
    "核となる主張\n",
    "- 本論文では、シーケンス生成タスクにおけるProximal Policy Optimization（PPO）の使用について、特に雑談チャットボットの文脈で議論する。\n",
    "- 著者らは、PPOは、これらのタスクで一般的に使用されているポリシー勾配と比較して、より効率的な強化学習アルゴリズムであると主張する。また、PPOの動的アプローチ（PPO-dynamic）を提案し、合成実験と雑談チャットボットのタスクでその有効性を実証する。\n",
    "\n",
    "証拠です：\n",
    "- PPO-dynamicは、合成カウントタスクにおいて、REINFORCEやMIXERなどの他のアルゴリズムと同等の高い精度スコアを達成しました。\n",
    "- 雑談チャットボットタスクでは、PPO-dynamicはREINFORCEやPPOよりもわずかに高いBLEU-2スコアを達成した。\n",
    "- PPOとPPO-dynamicの学習曲線は、policy gradientよりも安定しており、PPO-dynamicはより速く収束する。\n",
    "\n",
    "結論\n",
    "- PPOは、policy gradientと比較して、シーケンス学習の最適化手法として優れている。\n",
    "- PPO-dynamicはハイパーパラメータを動的に調整することにより、最適化プロセスをさらに改善する。\n",
    "- PPOは、GANベースのシーケンス学習において、より良いパフォーマンスを得るための新しい最適化手法として使用することができる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
