{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original: [openai-cookbook/examples/How_to_finetune_chat_models.ipynb at main · openai/openai-cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to fine-tune chat models\n",
    "\n",
    "このノートブックは、新しいgpt-3.5-turboのファインチューニングのためのガイドを提供します。\n",
    "\n",
    "[RecipeNLG](https://github.com/Glorf/recipenlg)データセットを使用し、エンティティ抽出を実行します。\n",
    "\n",
    "このデータセットは、様々なレシピと、それぞれから抽出された一般的な成分のリストを提供します。これは、固有表現認識（NER）タスクに共通のデータセットです。\n",
    "\n",
    "次の手順を実行します。\n",
    "\n",
    "1. セットアップ：データセットをロードし、ファインチューニングする1つのドメインに絞り込みます。\n",
    "2. データの準備：トレーニングと検証のサンプルを作成し、それらをファイルエンドポイントにアップロードすることで、ファインチューニング用のデータを準備します。\n",
    "3. ファインチューニング：ファインチューニングモデルを作成します。\n",
    "4. 推論：新しい入力の推論に微調整されたモデルを使用します。\n",
    "\n",
    "これが終わるまでに、ファインチューニングされたgpt-3.5-turboモデルをトレーニング、評価、デプロイできるようになります。\n",
    "\n",
    "ファインチューニングの詳細：\n",
    "\n",
    "- [Fine-tuning - OpenAI API](https://platform.openai.com/docs/guides/fine-tuning)\n",
    "- [API Reference - OpenAI API](https://platform.openai.com/docs/api-reference/fine-tuning)\n",
    "- [GPT-3.5 Turbo fine-tuning and API updates](https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in .\\.venv\\lib\\site-packages (0.27.9)\n",
      "Requirement already satisfied: aiohttp in .\\.venv\\lib\\site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: requests>=2.20 in .\\.venv\\lib\\site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in .\\.venv\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\.venv\\lib\\site-packages (from requests>=2.20->openai) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\.venv\\lib\\site-packages (from requests>=2.20->openai) (2.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\.venv\\lib\\site-packages (from requests>=2.20->openai) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\.venv\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in .\\.venv\\lib\\site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in .\\.venv\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in .\\.venv\\lib\\site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in .\\.venv\\lib\\site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in .\\.venv\\lib\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in .\\.venv\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: colorama in .\\.venv\\lib\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# openai pythonパッケージの最新バージョンを必ず使用する\n",
    "%pip install -U openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ファインチューニングは、特定のドメインに焦点を当てた場合に最適に機能します。データセットがモデルが学習できるように十分に焦点が絞られていると同時に、まだ見たことのないサンプルが見逃されない程度に一般的であることを確認することが重要です。\n",
    "\n",
    "これを念頭に置いて、 www.cookbooks.com のドキュメントのみを含むようにRecipeNLGデータセットからサブセットを抽出しました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>directions</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "      <th>NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No-Bake Nut Cookies</td>\n",
       "      <td>[\"1 c. firmly packed brown sugar\", \"1/2 c. eva...</td>\n",
       "      <td>[\"In a heavy 2-quart saucepan, mix brown sugar...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=44874</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "      <td>[\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jewell Ball'S Chicken</td>\n",
       "      <td>[\"1 small jar chipped beef, cut up\", \"4 boned ...</td>\n",
       "      <td>[\"Place chipped beef on bottom of baking dish....</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=699419</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "      <td>[\"beef\", \"chicken breasts\", \"cream of mushroom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creamy Corn</td>\n",
       "      <td>[\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...</td>\n",
       "      <td>[\"In a slow cooker, combine all ingredients. C...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=10570</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "      <td>[\"frozen corn\", \"cream cheese\", \"butter\", \"gar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chicken Funny</td>\n",
       "      <td>[\"1 large whole chicken\", \"2 (10 1/2 oz.) cans...</td>\n",
       "      <td>[\"Boil and debone chicken.\", \"Put bite size pi...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=897570</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "      <td>[\"chicken\", \"chicken gravy\", \"cream of mushroo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reeses Cups(Candy)</td>\n",
       "      <td>[\"1 c. peanut butter\", \"3/4 c. graham cracker ...</td>\n",
       "      <td>[\"Combine first four ingredients and press in ...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=659239</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "      <td>[\"peanut butter\", \"graham cracker crumbs\", \"bu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title                                        ingredients  \\\n",
       "0    No-Bake Nut Cookies  [\"1 c. firmly packed brown sugar\", \"1/2 c. eva...   \n",
       "1  Jewell Ball'S Chicken  [\"1 small jar chipped beef, cut up\", \"4 boned ...   \n",
       "2            Creamy Corn  [\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...   \n",
       "3          Chicken Funny  [\"1 large whole chicken\", \"2 (10 1/2 oz.) cans...   \n",
       "4   Reeses Cups(Candy)    [\"1 c. peanut butter\", \"3/4 c. graham cracker ...   \n",
       "\n",
       "                                          directions  \\\n",
       "0  [\"In a heavy 2-quart saucepan, mix brown sugar...   \n",
       "1  [\"Place chipped beef on bottom of baking dish....   \n",
       "2  [\"In a slow cooker, combine all ingredients. C...   \n",
       "3  [\"Boil and debone chicken.\", \"Put bite size pi...   \n",
       "4  [\"Combine first four ingredients and press in ...   \n",
       "\n",
       "                                              link             source  \\\n",
       "0   www.cookbooks.com/Recipe-Details.aspx?id=44874  www.cookbooks.com   \n",
       "1  www.cookbooks.com/Recipe-Details.aspx?id=699419  www.cookbooks.com   \n",
       "2   www.cookbooks.com/Recipe-Details.aspx?id=10570  www.cookbooks.com   \n",
       "3  www.cookbooks.com/Recipe-Details.aspx?id=897570  www.cookbooks.com   \n",
       "4  www.cookbooks.com/Recipe-Details.aspx?id=659239  www.cookbooks.com   \n",
       "\n",
       "                                                 NER  \n",
       "0  [\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"bu...  \n",
       "1  [\"beef\", \"chicken breasts\", \"cream of mushroom...  \n",
       "2  [\"frozen corn\", \"cream cheese\", \"butter\", \"gar...  \n",
       "3  [\"chicken\", \"chicken gravy\", \"cream of mushroo...  \n",
       "4  [\"peanut butter\", \"graham cracker crumbs\", \"bu...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# このタスクに使用するデータセットを読み込みます\n",
    "# これはRecipeNLGデータセットになります。 www.cookbooks.comのドキュメントのみを含むようにクリーンアップしました。\n",
    "recipe_df = pd.read_csv(\"./data/cookbook_recipes_nlg_10k.csv\")\n",
    "recipe_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの準備\n",
    "\n",
    "データを準備するところから始めます。\n",
    "\n",
    "ChatCompletion形式でファインチューニングする場合、各トレーニングサンプルはメッセージの単純なリストになります。たとえば、エントリは次のようになります。\n",
    "\n",
    "```\n",
    "[{'role': 'system',\n",
    "  'content': 'You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.'},\n",
    "```\n",
    "\n",
    "```\n",
    " {'role': 'user',\n",
    "  'content': 'Title: No-Bake Nut Cookies\\n\\nIngredients: [\"1 c. firmly packed brown sugar\", \"1/2 c. evaporated milk\", \"1/2 tsp. vanilla\", \"1/2 c. broken nuts (pecans)\", \"2 Tbsp. butter or margarine\", \"3 1/2 c. bite size shredded rice biscuits\"]\\n\\nGeneric ingredients: '},\n",
    "```\n",
    "\n",
    "```\n",
    " {'role': 'assistant',\n",
    "  'content': '[\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"butter\", \"bite size shredded rice biscuits\"]'}]\n",
    "```\n",
    "\n",
    "トレーニングプロセス中、この会話は分割され、最後のエントリはモデルが生成する完了であり、残りのメッセージはプロンプトとして機能します。\n",
    "\n",
    "トレーニングサンプルを作成するときは、これを考慮してください。モデルが複数ターンの会話で動作する場合は、会話が拡大し始めたときにパフォーマンスが低下しないように、代表的なサンプルを提供してください。\n",
    "\n",
    "現在、各トレーニングサンプルには4096トークンの制限があることに注意してください。これより長いものは4096トークンで切り捨てられます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'content': 'You are a helpful recipe assistant. You are to '\n",
      "                          'extract the generic ingredients from each of the '\n",
      "                          'recipes provided.',\n",
      "               'role': 'system'},\n",
      "              {'content': 'Title: No-Bake Nut Cookies\\n'\n",
      "                          '\\n'\n",
      "                          'Ingredients: [\"1 c. firmly packed brown sugar\", '\n",
      "                          '\"1/2 c. evaporated milk\", \"1/2 tsp. vanilla\", \"1/2 '\n",
      "                          'c. broken nuts (pecans)\", \"2 Tbsp. butter or '\n",
      "                          'margarine\", \"3 1/2 c. bite size shredded rice '\n",
      "                          'biscuits\"]\\n'\n",
      "                          '\\n'\n",
      "                          'Generic ingredients: ',\n",
      "               'role': 'user'},\n",
      "              {'content': '[\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", '\n",
      "                          '\"butter\", \"bite size shredded rice biscuits\"]',\n",
      "               'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "training_data = []\n",
    "\n",
    "system_message = \"You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.\"\n",
    "\n",
    "def prepare_example_conversation(row):\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "\n",
    "    user_message = f\"\"\"Title: {row['title']}\\n\\nIngredients: {row['ingredients']}\\n\\nGeneric ingredients: \"\"\"\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "    messages.append({\"role\": \"assistant\", \"content\": row[\"NER\"]})\n",
    "\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "pprint(prepare_example_conversation(recipe_df.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、トレーニングデータとして使用するデータセットのサブセットに対してこれを実行してみましょう。\n",
    "\n",
    "30~50この適切に剪定された例から始めることもできます。トレーニングセットのサイズを増やすと、パフォーマンスが直線的に拡大し続けることがわかりますが、ジョブの時間も長くなります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'role': 'system', 'content': 'You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.'}, {'role': 'user', 'content': 'Title: No-Bake Nut Cookies\\n\\nIngredients: [\"1 c. firmly packed brown sugar\", \"1/2 c. evaporated milk\", \"1/2 tsp. vanilla\", \"1/2 c. broken nuts (pecans)\", \"2 Tbsp. butter or margarine\", \"3 1/2 c. bite size shredded rice biscuits\"]\\n\\nGeneric ingredients: '}, {'role': 'assistant', 'content': '[\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"butter\", \"bite size shredded rice biscuits\"]'}]}\n",
      "{'messages': [{'role': 'system', 'content': 'You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.'}, {'role': 'user', 'content': 'Title: Jewell Ball\\'S Chicken\\n\\nIngredients: [\"1 small jar chipped beef, cut up\", \"4 boned chicken breasts\", \"1 can cream of mushroom soup\", \"1 carton sour cream\"]\\n\\nGeneric ingredients: '}, {'role': 'assistant', 'content': '[\"beef\", \"chicken breasts\", \"cream of mushroom soup\", \"sour cream\"]'}]}\n",
      "{'messages': [{'role': 'system', 'content': 'You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.'}, {'role': 'user', 'content': 'Title: Creamy Corn\\n\\nIngredients: [\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg. cream cheese, cubed\", \"1/3 c. butter, cubed\", \"1/2 tsp. garlic powder\", \"1/2 tsp. salt\", \"1/4 tsp. pepper\"]\\n\\nGeneric ingredients: '}, {'role': 'assistant', 'content': '[\"frozen corn\", \"cream cheese\", \"butter\", \"garlic powder\", \"salt\", \"pepper\"]'}]}\n",
      "{'messages': [{'role': 'system', 'content': 'You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.'}, {'role': 'user', 'content': 'Title: Chicken Funny\\n\\nIngredients: [\"1 large whole chicken\", \"2 (10 1/2 oz.) cans chicken gravy\", \"1 (10 1/2 oz.) can cream of mushroom soup\", \"1 (6 oz.) box Stove Top stuffing\", \"4 oz. shredded cheese\"]\\n\\nGeneric ingredients: '}, {'role': 'assistant', 'content': '[\"chicken\", \"chicken gravy\", \"cream of mushroom soup\", \"shredded cheese\"]'}]}\n",
      "{'messages': [{'role': 'system', 'content': 'You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.'}, {'role': 'user', 'content': 'Title: Reeses Cups(Candy)  \\n\\nIngredients: [\"1 c. peanut butter\", \"3/4 c. graham cracker crumbs\", \"1 c. melted butter\", \"1 lb. (3 1/2 c.) powdered sugar\", \"1 large pkg. chocolate chips\"]\\n\\nGeneric ingredients: '}, {'role': 'assistant', 'content': '[\"peanut butter\", \"graham cracker crumbs\", \"butter\", \"powdered sugar\", \"chocolate chips\"]'}]}\n"
     ]
    }
   ],
   "source": [
    "# データセットの最初の100行をトレーニングに使用します\n",
    "training_df = recipe_df.loc[0:100]\n",
    "\n",
    "# prepare_example_conversation関数をtraining_dfの各行に適用します\n",
    "training_data = training_df.apply(prepare_example_conversation, axis=1).to_list()\n",
    "\n",
    "for example in training_data[:5]:\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トレーニングデータに加えて、モデルがトレーニングセットに過剰適合していないことを確認するために使用される検証データもオプションで提供できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df = recipe_df.loc[101:200]\n",
    "validation_data = validation_df.apply(prepare_example_conversation, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、データを`.jsonl`ファイルとして保存する必要があります。各行は1つのトレーニング会話例です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsonl(data_list: list, filename: str) -> None:\n",
    "    with open(filename, \"w\") as out:\n",
    "        for ddict in data_list:\n",
    "            jout = json.dumps(ddict) + \"\\n\"\n",
    "            out.write(jout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_name = \"tmp_recipe_finetune_training.jsonl\"\n",
    "write_jsonl(training_data, training_file_name)\n",
    "\n",
    "validation_file_name = \"tmp_recipe_finetune_validation.jsonl\"\n",
    "write_jsonl(validation_data, validation_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トレーニング`.jsonl`ファイルの最初の5行は次のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.\"}, {\"role\": \"user\", \"content\": \"Title: No-Bake Nut Cookies\\n\\nIngredients: [\\\"1 c. firmly packed brown sugar\\\", \\\"1/2 c. evaporated milk\\\", \\\"1/2 tsp. vanilla\\\", \\\"1/2 c. broken nuts (pecans)\\\", \\\"2 Tbsp. butter or margarine\\\", \\\"3 1/2 c. bite size shredded rice biscuits\\\"]\\n\\nGeneric ingredients: \"}, {\"role\": \"assistant\", \"content\": \"[\\\"brown sugar\\\", \\\"milk\\\", \\\"vanilla\\\", \\\"nuts\\\", \\\"butter\\\", \\\"bite size shredded rice biscuits\\\"]\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.\"}, {\"role\": \"user\", \"content\": \"Title: Jewell Ball'S Chicken\\n\\nIngredients: [\\\"1 small jar chipped beef, cut up\\\", \\\"4 boned chicken breasts\\\", \\\"1 can cream of mushroom soup\\\", \\\"1 carton sour cream\\\"]\\n\\nGeneric ingredients: \"}, {\"role\": \"assistant\", \"content\": \"[\\\"beef\\\", \\\"chicken breasts\\\", \\\"cream of mushroom soup\\\", \\\"sour cream\\\"]\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.\"}, {\"role\": \"user\", \"content\": \"Title: Creamy Corn\\n\\nIngredients: [\\\"2 (16 oz.) pkg. frozen corn\\\", \\\"1 (8 oz.) pkg. cream cheese, cubed\\\", \\\"1/3 c. butter, cubed\\\", \\\"1/2 tsp. garlic powder\\\", \\\"1/2 tsp. salt\\\", \\\"1/4 tsp. pepper\\\"]\\n\\nGeneric ingredients: \"}, {\"role\": \"assistant\", \"content\": \"[\\\"frozen corn\\\", \\\"cream cheese\\\", \\\"butter\\\", \\\"garlic powder\\\", \\\"salt\\\", \\\"pepper\\\"]\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.\"}, {\"role\": \"user\", \"content\": \"Title: Chicken Funny\\n\\nIngredients: [\\\"1 large whole chicken\\\", \\\"2 (10 1/2 oz.) cans chicken gravy\\\", \\\"1 (10 1/2 oz.) can cream of mushroom soup\\\", \\\"1 (6 oz.) box Stove Top stuffing\\\", \\\"4 oz. shredded cheese\\\"]\\n\\nGeneric ingredients: \"}, {\"role\": \"assistant\", \"content\": \"[\\\"chicken\\\", \\\"chicken gravy\\\", \\\"cream of mushroom soup\\\", \\\"shredded cheese\\\"]\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.\"}, {\"role\": \"user\", \"content\": \"Title: Reeses Cups(Candy)  \\n\\nIngredients: [\\\"1 c. peanut butter\\\", \\\"3/4 c. graham cracker crumbs\\\", \\\"1 c. melted butter\\\", \\\"1 lb. (3 1/2 c.) powdered sugar\\\", \\\"1 large pkg. chocolate chips\\\"]\\n\\nGeneric ingredients: \"}, {\"role\": \"assistant\", \"content\": \"[\\\"peanut butter\\\", \\\"graham cracker crumbs\\\", \\\"butter\\\", \\\"powdered sugar\\\", \\\"chocolate chips\\\"]\"}]}\n"
     ]
    }
   ],
   "source": [
    "# トレーニングファイルの最初の5行を出力します\n",
    "# for linux\n",
    "# !head -n 5 tmp_recipe_finetune_training.jsonl\n",
    "# for windows (powershell)\n",
    "!Powershell.exe -Command \"type .\\tmp_recipe_finetune_training.jsonl -Head 5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ファイルのアップロード\n",
    "\n",
    "これで、ファイルをファイルエンドポイントにアップロードして、微調整されたモデルで使用できるようになりました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file ID:  file-OCWzCknyiRyzN0Y0FwPFehWv\n",
      "Vaidation file ID:  file-ZO9wdLuzneUDkVouldsvQeG2\n"
     ]
    }
   ],
   "source": [
    "training_response = openai.File.create(\n",
    "    file=open(training_file_name, \"rb\"), purpose=\"fine-tune\"\n",
    ")\n",
    "training_file_id = training_response[\"id\"]\n",
    "\n",
    "validation_response = openai.File.create(\n",
    "    file=open(validation_file_name, \"rb\"), purpose=\"fine-tune\"\n",
    ")\n",
    "validation_file_id = validation_response[\"id\"]\n",
    "\n",
    "print(\"Training file ID: \", training_file_id)\n",
    "print(\"Vaidation file ID: \", validation_file_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ファインチューニング\n",
    "\n",
    "これで、生成されたファイルとモデルを識別するためのオプションのサフィックスを使用して、ファインチューニングジョブを作成できます。応答には、ジョブの更新を取得するために使用できるIDが含まれます。\n",
    "\n",
    "注：ファイルは最初にシステムによって処理される必要があるため、「ファイルの準備ができていません」エラーが発生する可能性があります。その場合は、数分後に再試行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-Yjrf3FgY0XhYavCXO8N44wed\n",
      "Status: created\n"
     ]
    }
   ],
   "source": [
    "response = openai.FineTuningJob.create(\n",
    "    training_file=training_file_id,\n",
    "    validation_file=validation_file_id,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    suffix=\"recipe-ner\"\n",
    ")\n",
    "\n",
    "job_id = response[\"id\"]\n",
    "\n",
    "print(\"Job ID:\", response[\"id\"])\n",
    "print(\"Status:\", response[\"status\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ジョブのステータスをチェック\n",
    "\n",
    "https://api.openai.com/v1/alpha/fine-tunes エンドポイントに`GET`リクエストを送信して、アルファファインチューニングジョブを一覧表示できます。この例では、前の手順で取得したIDが`status: succeeded`になることを確認する必要があります。\n",
    "\n",
    "完了したら、`result_files`を使用して検証セットから結果をサンプリングし（検証セットをアップロードした場合）、`fine_tuned_model`パラメーターのIDを使用してトレーニング済モデルを呼び出すことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-Yjrf3FgY0XhYavCXO8N44wed\n",
      "Status: running\n",
      "Trained Tokens: None\n"
     ]
    }
   ],
   "source": [
    "response = openai.FineTuningJob.retrieve(job_id)\n",
    "\n",
    "print(\"Job ID:\", response[\"id\"])\n",
    "print(\"Status:\", response[\"status\"])\n",
    "print(\"Trained Tokens:\", response[\"trained_tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "イベントエンドポイントを使用してファインチューニングの進行状況を追跡できます。ファインチューニングの準備が完了するまで、以下のセルを数回再実行できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created fine-tune: ftjob-Yjrf3FgY0XhYavCXO8N44wed\n",
      "Fine tuning job started\n",
      "Step 10: training loss=0.07\n",
      "Step 20: training loss=0.01\n",
      "Step 30: training loss=0.10\n",
      "Step 40: training loss=0.00\n",
      "Step 50: training loss=0.03\n",
      "Step 60: training loss=0.15\n",
      "Step 70: training loss=0.18\n",
      "Step 80: training loss=0.53\n",
      "Step 90: training loss=0.01\n",
      "Step 100: training loss=0.05\n",
      "Step 110: training loss=0.07\n",
      "Step 120: training loss=0.00\n",
      "Step 130: training loss=0.07\n",
      "Step 140: training loss=0.00\n",
      "Step 150: training loss=0.00\n",
      "Step 160: training loss=0.30\n",
      "Step 170: training loss=0.80\n",
      "Step 180: training loss=0.00\n",
      "Step 190: training loss=0.00\n",
      "Step 200: training loss=0.21\n",
      "Step 210: training loss=0.00\n",
      "Step 220: training loss=0.45\n",
      "Step 230: training loss=1.18\n",
      "Step 240: training loss=0.00\n",
      "Step 250: training loss=0.00\n",
      "Step 260: training loss=0.26\n",
      "Step 270: training loss=0.00\n",
      "Step 280: training loss=0.00\n",
      "Step 290: training loss=0.00\n",
      "Step 300: training loss=0.00\n",
      "New fine-tuned model created: ft:gpt-3.5-turbo-0613:ijiwarunahello:recipe-ner:7qsfhZfd\n",
      "Fine-tuning job successfully completed\n"
     ]
    }
   ],
   "source": [
    "response = openai.FineTuningJob.list_events(id=job_id, limit=50)\n",
    "\n",
    "events = response[\"data\"]\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event[\"message\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、ジョブからファインチューニングされたモデルIDを取得できるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model ID: ft:gpt-3.5-turbo-0613:ijiwarunahello:recipe-ner:7qsfhZfd\n"
     ]
    }
   ],
   "source": [
    "response = openai.FineTuningJob.retrieve(job_id)\n",
    "fine_tuned_model_id = response[\"fine_tuned_model\"]\n",
    "\n",
    "print(\"Fine-tuned model ID:\", fine_tuned_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論\n",
    "\n",
    "最後のステップは、ファインチューニングしたモデルを推論に使用することです。従来の`FineTuning`と同様に、新しいファインチューニングされたモデル名をモデルパラメータに入力して`ChatCompletions`を呼び出すだけです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'system', 'content': 'You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.'}\n"
     ]
    }
   ],
   "source": [
    "print(training_data[-1][\"messages\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are a helpful recipe assistant. You are to extract the '\n",
      "             'generic ingredients from each of the recipes provided.',\n",
      "  'role': 'system'},\n",
      " {'content': 'Title: Pancakes\\n'\n",
      "             '\\n'\n",
      "             'Ingredients: [\"1 c. flour\", \"1 tsp. soda\", \"1 tsp. salt\", \"1 '\n",
      "             'Tbsp. sugar\", \"1 egg\", \"3 Tbsp. margarine, melted\", \"1 c. '\n",
      "             'buttermilk\"]\\n'\n",
      "             '\\n'\n",
      "             'Generic ingredients: ',\n",
      "  'role': 'user'}]\n"
     ]
    }
   ],
   "source": [
    "# cookbookの実装では動かない（2023.08.24現在）\n",
    "# test_row = test_df.iloc[0]\n",
    "# test_messages = []\n",
    "# test_messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "# user_message = create_user_message(test_row)\n",
    "# test_messages.append({\"role\": \"user\", \"content\": create_user_message(test_row)})\n",
    "\n",
    "# 以下で対処\n",
    "def create_user_message(data):\n",
    "    user_message = {}\n",
    "    messages = data[\"messages\"]\n",
    "    for message in messages:\n",
    "        if message.get(\"role\", None) == \"user\":\n",
    "            user_message = message\n",
    "            break\n",
    "    return user_message\n",
    "\n",
    "test_data = training_data[-1]\n",
    "test_messages = []\n",
    "test_messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "user_message = create_user_message(test_data)\n",
    "test_messages.append(user_message)\n",
    "\n",
    "pprint(test_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"flour\", \"soda\", \"salt\", \"sugar\", \"egg\", \"margarine\", \"buttermilk\"]\n"
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=fine_tuned_model_id, messages=test_messages, temperature=0, max_tokens=500\n",
    ")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## オプション：データフォーマットの確認\n",
    "\n",
    "参考：[Fine-tuning - OpenAI API](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset)\n",
    "\n",
    "データセットをコンパイルし、ファインチューニングジョブを作成する前に、データのフォーマットをチェックすることが重要です。\n",
    "\n",
    "- 潜在的なエラーの発見\n",
    "- トークン数の確認\n",
    "- ファインチューニングジョブのコストの見積もり"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in .\\.venv\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in .\\.venv\\lib\\site-packages (from tiktoken) (2023.8.8)\n",
      "Requirement already satisfied: requests>=2.26.0 in .\\.venv\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def check_data_formatting(data):\n",
    "    # データフォーマットのチェック\n",
    "    # 例数と最初の項目をチェックすることで、データを素早く検査することができる\n",
    "    print(\"Num examples: \", len(data))\n",
    "    print(\"First example:\")\n",
    "    for message in data[0][\"messages\"]:\n",
    "        print(message)\n",
    "\n",
    "    # データの感覚を掴んだところで、すべての異なる例を見て、書式が正しいか、チャットの完了メッセージの構造と一致しているかをチェックする必要があります。\n",
    "\n",
    "    # フォーマットエラーのチェック\n",
    "    format_errors = defaultdict(int)\n",
    "    for ex in data:\n",
    "        if not isinstance(ex, dict):\n",
    "            format_errors[\"data_type\"] += 1\n",
    "            continue\n",
    "\n",
    "        messages = ex.get(\"messages\", None)\n",
    "        if not messages:\n",
    "            format_errors[\"missing_message_list\"] += 1\n",
    "            continue\n",
    "\n",
    "        for message in messages:\n",
    "            if \"role\" not in message or \"content\" not in message:\n",
    "                format_errors[\"message_missing_key\"] += 1\n",
    "\n",
    "            if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
    "                format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "            if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
    "                format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "            content = message.get(\"content\", None)\n",
    "            if not content or not isinstance(content, str):\n",
    "                format_errors[\"missing_content\"] += 1\n",
    "\n",
    "        if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "            format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "    if format_errors:\n",
    "        print(\"Found errors:\")\n",
    "        for k, v in format_errors.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "    else:\n",
    "        print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# メッセージの構造だけでなく、長さが4096トークンの制限を超えないようにする必要もあります。\n",
    "\n",
    "# トークンカウント関数\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# 正確ではないことに注意する\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
    "\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_EPOCHS = 1\n",
    "MAX_EPOCHS = 25\n",
    "\n",
    "def warnings_and_tokens_count(dataset):\n",
    "    n_missing_system = 0\n",
    "    n_missing_user = 0\n",
    "    n_messages = []\n",
    "    convo_lens = []\n",
    "    assistant_message_lens = []\n",
    "\n",
    "    for ex in dataset:\n",
    "        messages = ex[\"messages\"]\n",
    "        if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "            n_missing_system += 1\n",
    "        if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "            n_missing_user += 1\n",
    "        n_messages.append(len(messages))\n",
    "        convo_lens.append(num_tokens_from_messages(messages))\n",
    "        assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "    print(\"Num examples missing system message:\", n_missing_system)\n",
    "    print(\"Num examples missing user message:\", n_missing_user)\n",
    "    print_distribution(n_messages, \"num_messages_per_example\")\n",
    "    print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "    print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "    n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "    print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")\n",
    "\n",
    "\n",
    "    # pricing and default n_epochs estimate\n",
    "    n_epochs = TARGET_EPOCHS\n",
    "    n_train_examples = len(dataset)\n",
    "    if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "        n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "    elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "        n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "    n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "    print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "    print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "    print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "    print(\"See pricing page to estimate total costs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples:  101\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.'}\n",
      "{'role': 'user', 'content': 'Title: No-Bake Nut Cookies\\n\\nIngredients: [\"1 c. firmly packed brown sugar\", \"1/2 c. evaporated milk\", \"1/2 tsp. vanilla\", \"1/2 c. broken nuts (pecans)\", \"2 Tbsp. butter or margarine\", \"3 1/2 c. bite size shredded rice biscuits\"]\\n\\nGeneric ingredients: '}\n",
      "{'role': 'assistant', 'content': '[\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"butter\", \"bite size shredded rice biscuits\"]'}\n",
      "No errors found\n",
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 69, 227\n",
      "mean / median: 134.16831683168317, 130.0\n",
      "p5 / p95: 97.0, 180.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 8, 58\n",
      "mean / median: 28.15841584158416, 26.0\n",
      "p5 / p95: 16.0, 43.0\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n",
      "Dataset has ~13551 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~40653 tokens\n",
      "See pricing page to estimate total costs\n"
     ]
    }
   ],
   "source": [
    "check_data_formatting(training_data)\n",
    "warnings_and_tokens_count(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples:  100\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.'}\n",
      "{'role': 'user', 'content': 'Title: Crustless Vegetable Ham Pie\\n\\nIngredients: [\"1/4 c. butter\", \"1/4 lb. mushrooms, sliced\", \"1 garlic clove, minced\", \"1 medium zucchini, sliced thinly\", \"1/4 c. chopped onion\", \"1 c. diced, cooked ham\", \"4 eggs\", \"2 c. Ricotta cheese\", \"1 c. shredded Monterey Jack\", \"1 (10 oz.) pkg. frozen spinach, thawed and drained\", \"1/2 tsp. dill weed\", \"salt and pepper\"]\\n\\nGeneric ingredients: '}\n",
      "{'role': 'assistant', 'content': '[\"butter\", \"mushrooms\", \"garlic\", \"zucchini\", \"onion\", \"eggs\", \"Ricotta cheese\", \"shredded Monterey Jack\", \"frozen spinach\", \"dill weed\", \"salt\"]'}\n",
      "No errors found\n",
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 73, 276\n",
      "mean / median: 145.0, 139.5\n",
      "p5 / p95: 102.0, 202.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 10, 69\n",
      "mean / median: 31.39, 29.0\n",
      "p5 / p95: 17.0, 50.0\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n",
      "Dataset has ~14500 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~43500 tokens\n",
      "See pricing page to estimate total costs\n"
     ]
    }
   ],
   "source": [
    "check_data_formatting(validation_data)\n",
    "warnings_and_tokens_count(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考情報\n",
    "\n",
    "- [固有表現抽出（NER）とはどんなタスクか【自然言語処理タスク紹介1】 | もじとばコム](https://mojitoba.com/2021/03/03/what_is_ner/)\n",
    "- [【python】jsonっぽいファイル「jsonl（JSON LINES）」をcsvに書き換えるまで｜じゅにこ](https://note.com/junico02_se_blog/n/n5ed9c474aed4)\n",
    "- [RecipeNLG](https://recipenlg.cs.put.poznan.pl/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
